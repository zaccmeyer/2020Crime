{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import itertools as it\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DR_NO', 'Date Rptd', 'DATE OCC', 'TIME OCC', 'AREA', 'AREA NAME', 'Rpt Dist No', 'Part 1-2', 'Mocodes', 'Vict Age', 'Vict Sex', 'Premis Cd', 'Premis Desc', 'Weapon Used Cd', 'Weapon Desc', 'LOCATION', 'Cross Street', 'LAT', 'LON', 'Crm Cd 1']\n",
      "67176\n"
     ]
    }
   ],
   "source": [
    "#DATA CLEANING CELL\n",
    "crime_data = pd.read_csv('Crime_Data_from_2020_to_Present.csv')\n",
    "# print(len(my_data.columns))\n",
    "crime_data = crime_data.drop(columns=['Vict Descent','Status','Status Desc'])\n",
    "# print(len(my_data.columns))\n",
    "all_c = list(crime_data.columns)\n",
    "final = []\n",
    "for i in all_c:\n",
    "    if 'Crm' not in i:\n",
    "        final.append(i)\n",
    "final.append('Crm Cd 1')\n",
    "print(final)\n",
    "crime_data.dropna(inplace=True,subset=final)\n",
    "crime_data = crime_data[crime_data['LON'] != 0]\n",
    "crime_data = crime_data[crime_data['LAT'] != 0]\n",
    "\n",
    "print(len(crime_data))\n",
    "# CODE TO DROP CRIMES WITH ONLY A FEW INCIDENTS\n",
    "X = crime_data.drop(columns=['Crm Cd 1', 'Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4', 'Crm Cd'])\n",
    "y = crime_data['Crm Cd']\n",
    "# allowed number of crime incidents to keep in data\n",
    "num_incidents = 100\n",
    "crimes = []\n",
    "for i in set(y):\n",
    "    if len(crime_data[crime_data['Crm Cd']==i])>num_incidents:\n",
    "        crimes.append(i)\n",
    "\n",
    "new_df = crime_data[crime_data['Crm Cd'].isin(crimes)]\n",
    "new_df = pd.get_dummies(new_df, drop_first=True, columns=[\"Vict Sex\"],dtype = int)\n",
    "new_df['Vict Sex'] = new_df['Vict Sex_H'] + 2*new_df['Vict Sex_M'] + 3*new_df['Vict Sex_X']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DR_NO',\n",
       " 'Date Rptd',\n",
       " 'DATE OCC',\n",
       " 'TIME OCC',\n",
       " 'AREA',\n",
       " 'AREA NAME',\n",
       " 'Rpt Dist No',\n",
       " 'Part 1-2',\n",
       " 'Crm Cd',\n",
       " 'Crm Cd Desc',\n",
       " 'Mocodes',\n",
       " 'Vict Age',\n",
       " 'Premis Cd',\n",
       " 'Premis Desc',\n",
       " 'Weapon Used Cd',\n",
       " 'Weapon Desc',\n",
       " 'Crm Cd 1',\n",
       " 'Crm Cd 2',\n",
       " 'Crm Cd 3',\n",
       " 'Crm Cd 4',\n",
       " 'LOCATION',\n",
       " 'Cross Street',\n",
       " 'LAT',\n",
       " 'LON',\n",
       " 'Vict Sex_H',\n",
       " 'Vict Sex_M',\n",
       " 'Vict Sex_X',\n",
       " 'Vict Sex']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(new_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['TIME OCC','AREA','Rpt Dist No','Vict Age','Vict Sex','Weapon Used Cd','LAT','LON']\n",
    "labels = new_df['Crm Cd 1']\n",
    "X, X_test, y, y_test = train_test_split(new_df[features], labels,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best(criterion=\"aic\"):\n",
    "    features = ['TIME OCC','AREA','Rpt Dist No','Vict Age','Vict Sex','Weapon Used Cd','LAT','LON']\n",
    "    labels = new_df['Crm Cd 1']\n",
    "    X, X_test, y, y_test = train_test_split(new_df[features], labels,random_state=2)\n",
    "    train = X\n",
    "    X1 = train[features].values\n",
    "    results = GradientBoostingClassifier(max_features=15).fit(X1,y)\n",
    "    crit = results.score(X_test[features],y_test)\n",
    "    best_score = crit\n",
    "    best_f = features\n",
    "\n",
    "    for k in tqdm(range(1,len(features))):\n",
    "        for combo in it.combinations(features,k):\n",
    "            # print(combo)\n",
    "            # print(train[list(combo)])\n",
    "            X1 = train[list(combo)].values\n",
    "            results = GradientBoostingClassifier(max_features=15).fit(X1,y)\n",
    "            crit = results.score(X_test[list(combo)],y_test)\n",
    "            if crit > best_score:\n",
    "                best_score = crit\n",
    "                best_f = combo\n",
    "    return best_score,best_f\n",
    "    # print(f\"best: {best[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      " 14%|█▍        | 1/7 [27:36<2:45:36, 1656.07s/it]/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/mckayladavis/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      " 14%|█▍        | 1/7 [2:18:01<13:48:11, 8281.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_score, best_f \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 17\u001b[0m, in \u001b[0;36mfind_best\u001b[0;34m(criterion)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m combo \u001b[38;5;129;01min\u001b[39;00m it\u001b[38;5;241m.\u001b[39mcombinations(features,k):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# print(combo)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# print(train[list(combo)])\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     X1 \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;28mlist\u001b[39m(combo)]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 17\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mGradientBoostingClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     crit \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mscore(X_test[\u001b[38;5;28mlist\u001b[39m(combo)],y_test)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m crit \u001b[38;5;241m>\u001b[39m best_score:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:525\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resize_state()\n\u001b[1;32m    524\u001b[0m \u001b[39m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 525\u001b[0m n_stages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_stages(\n\u001b[1;32m    526\u001b[0m     X,\n\u001b[1;32m    527\u001b[0m     y,\n\u001b[1;32m    528\u001b[0m     raw_predictions,\n\u001b[1;32m    529\u001b[0m     sample_weight,\n\u001b[1;32m    530\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rng,\n\u001b[1;32m    531\u001b[0m     X_val,\n\u001b[1;32m    532\u001b[0m     y_val,\n\u001b[1;32m    533\u001b[0m     sample_weight_val,\n\u001b[1;32m    534\u001b[0m     begin_at_stage,\n\u001b[1;32m    535\u001b[0m     monitor,\n\u001b[1;32m    536\u001b[0m )\n\u001b[1;32m    538\u001b[0m \u001b[39m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m n_stages \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:603\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    596\u001b[0m         initial_loss \u001b[39m=\u001b[39m loss_(\n\u001b[1;32m    597\u001b[0m             y[\u001b[39m~\u001b[39msample_mask],\n\u001b[1;32m    598\u001b[0m             raw_predictions[\u001b[39m~\u001b[39msample_mask],\n\u001b[1;32m    599\u001b[0m             sample_weight[\u001b[39m~\u001b[39msample_mask],\n\u001b[1;32m    600\u001b[0m         )\n\u001b[1;32m    602\u001b[0m \u001b[39m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m raw_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_stage(\n\u001b[1;32m    604\u001b[0m     i,\n\u001b[1;32m    605\u001b[0m     X,\n\u001b[1;32m    606\u001b[0m     y,\n\u001b[1;32m    607\u001b[0m     raw_predictions,\n\u001b[1;32m    608\u001b[0m     sample_weight,\n\u001b[1;32m    609\u001b[0m     sample_mask,\n\u001b[1;32m    610\u001b[0m     random_state,\n\u001b[1;32m    611\u001b[0m     X_csc,\n\u001b[1;32m    612\u001b[0m     X_csr,\n\u001b[1;32m    613\u001b[0m )\n\u001b[1;32m    615\u001b[0m \u001b[39m# track loss\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    242\u001b[0m     sample_weight \u001b[39m=\u001b[39m sample_weight \u001b[39m*\u001b[39m sample_mask\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    244\u001b[0m X \u001b[39m=\u001b[39m X_csr \u001b[39mif\u001b[39;00m X_csr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\n\u001b[0;32m--> 245\u001b[0m tree\u001b[39m.\u001b[39;49mfit(X, residual, sample_weight\u001b[39m=\u001b[39;49msample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    247\u001b[0m \u001b[39m# update tree leaves\u001b[39;00m\n\u001b[1;32m    248\u001b[0m loss\u001b[39m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    249\u001b[0m     tree\u001b[39m.\u001b[39mtree_,\n\u001b[1;32m    250\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     k\u001b[39m=\u001b[39mk,\n\u001b[1;32m    258\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/tree/_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1292\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \n\u001b[1;32m   1294\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m   1321\u001b[0m         X,\n\u001b[1;32m   1322\u001b[0m         y,\n\u001b[1;32m   1323\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1324\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m   1325\u001b[0m     )\n\u001b[1;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/acme/lib/python3.10/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score, best_f = find_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(new_df[features], labels,random_state=2)\n",
    "X = X[best_f]\n",
    "X_test = X_test[best_f]\n",
    "\n",
    "loss = ['log_loss']\n",
    "n_estimators = [50,100]\n",
    "max_depth = [1,9]\n",
    "min_samples_leaf = [1,4,8]\n",
    "\n",
    "max_loss = 0\n",
    "max_n = 0\n",
    "max_d = 0\n",
    "max_leaf = 0\n",
    "best_score = 0\n",
    "\n",
    "for l in loss:\n",
    "  for n in n_estimators:\n",
    "    for md in max_depth:\n",
    "      for ml in min_samples_leaf:\n",
    "        model = GradientBoostingClassifier(loss = l, n_estimators = n, max_depth = md, min_samples_leaf = ml,max_features=10)\n",
    "        model.fit(X,y)\n",
    "        score = model.score(X_test,y_test)\n",
    "        if score > best_score:\n",
    "          max_loss = l\n",
    "          max_n = n\n",
    "          max_d = md\n",
    "          max_leaf = ml\n",
    "          best_score = score\n",
    "print(\"Best Loss:\", max_loss)\n",
    "print(\"Best n_estimators:\", max_n)\n",
    "print(\"Best max_d:\", md)\n",
    "print(\"Best min_samples_leaf:\", max_leaf)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ['quantile', 'huber', 'squared_error', 'absolute_error']\n",
    "n_estimators = [50,100]\n",
    "max_depth = [1,9]\n",
    "min_samples_leaf = [1,4,8]\n",
    "\n",
    "max_loss = 0\n",
    "max_n = 0\n",
    "max_d = 0\n",
    "max_leaf = 0\n",
    "best_score = 0\n",
    "\n",
    "for l in loss:\n",
    "  for n in n_estimators:\n",
    "    for md in max_depth:\n",
    "      for ml in min_samples_leaf:\n",
    "        model = GradientBoostingRegressor(loss = l, n_estimators = n, max_depth = md, min_samples_leaf = ml,max_features=10)\n",
    "        model.fit(X,y)\n",
    "        score = model.score(X_test,y_test)\n",
    "        if score > best_score:\n",
    "          max_loss = l\n",
    "          max_n = n\n",
    "          max_d = md\n",
    "          max_leaf = ml\n",
    "          best_score = score\n",
    "print(\"Best Loss:\", max_loss)\n",
    "print(\"Best n_estimators:\", max_n)\n",
    "print(\"Best max_d:\", md)\n",
    "print(\"Best min_samples_leaf:\", max_leaf)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = xgb.XGBClassifier(objective = 'multi:softmax')\n",
    "# Specify values for certain hyperparameters\n",
    "param_grid = {\"alpha\": [1,3,5],\n",
    "                 \"gamma\": [1,3,5],\n",
    "                 \"lambda\": [3,5,7],\n",
    "                 \"eta\": [.3,.5,.9]}\n",
    "knn_gs = GridSearchCV(knn, param_grid,cv=2)\n",
    "# Run the actual search. This may take some time.\n",
    "knn_gs.fit(X, y)\n",
    "# After fitting, you can access data about the results.\n",
    "print('best paramaters: ' + str(knn_gs.best_params_), 'best score: ' +str(knn_gs.best_score_), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('acme')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9667e94c5242b6e179d4931e51f0ea8382a201d07ce0883b2e989376c502410"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
